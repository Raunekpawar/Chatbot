{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68ea9af0-b214-46e7-86bd-0490be4e46fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d3fc9b9-0dcb-45e7-a35e-88b465543ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, num_classes, img_shape):\n",
    "        super(Generator, self).__init__()\n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim + num_classes, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.img_shape = img_shape\n",
    "\n",
    "    def forward(self, z, labels):\n",
    "        gen_input = torch.cat((z, self.label_emb(labels)), -1)\n",
    "        img = self.model(gen_input)\n",
    "        img = img.view(img.size(0), *self.img_shape)\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0011423a-cb92-458d-84a4-40d46762a1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_classes, img_shape):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(img_shape)) + num_classes, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        d_input = torch.cat((img.view(img.size(0), -1), self.label_emb(labels)), -1)\n",
    "        validity = self.model(d_input)\n",
    "        return validity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7551810e-7771-4914-aa7d-b72765c854e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50] | D Loss: 0.1475 | G Loss: 3.0319\n",
      "Epoch [2/50] | D Loss: 0.2876 | G Loss: 3.4285\n",
      "Epoch [3/50] | D Loss: 0.4485 | G Loss: 1.9518\n",
      "Epoch [4/50] | D Loss: 0.7276 | G Loss: 3.8470\n",
      "Epoch [5/50] | D Loss: 0.2845 | G Loss: 2.2963\n",
      "Epoch [6/50] | D Loss: 0.1322 | G Loss: 3.6823\n",
      "Epoch [7/50] | D Loss: 0.2187 | G Loss: 3.4079\n",
      "Epoch [8/50] | D Loss: 0.0678 | G Loss: 4.4234\n",
      "Epoch [9/50] | D Loss: 0.0982 | G Loss: 5.1831\n",
      "Epoch [10/50] | D Loss: 0.0744 | G Loss: 4.1677\n",
      "Epoch [11/50] | D Loss: 0.2181 | G Loss: 3.7302\n",
      "Epoch [12/50] | D Loss: 0.1228 | G Loss: 2.9255\n",
      "Epoch [13/50] | D Loss: 0.0995 | G Loss: 4.9800\n",
      "Epoch [14/50] | D Loss: 0.2097 | G Loss: 3.0810\n",
      "Epoch [15/50] | D Loss: 0.0802 | G Loss: 3.6244\n",
      "Epoch [16/50] | D Loss: 0.1530 | G Loss: 3.1879\n",
      "Epoch [17/50] | D Loss: 0.1684 | G Loss: 3.4257\n",
      "Epoch [18/50] | D Loss: 0.2246 | G Loss: 2.6944\n",
      "Epoch [19/50] | D Loss: 0.1221 | G Loss: 3.8075\n",
      "Epoch [20/50] | D Loss: 0.2353 | G Loss: 3.0551\n",
      "Epoch [21/50] | D Loss: 0.2824 | G Loss: 4.0538\n",
      "Epoch [22/50] | D Loss: 0.3427 | G Loss: 3.0482\n",
      "Epoch [23/50] | D Loss: 0.3463 | G Loss: 3.2163\n",
      "Epoch [24/50] | D Loss: 0.1809 | G Loss: 2.9642\n",
      "Epoch [25/50] | D Loss: 0.2682 | G Loss: 3.3453\n",
      "Epoch [26/50] | D Loss: 0.1522 | G Loss: 2.9588\n",
      "Epoch [27/50] | D Loss: 0.3270 | G Loss: 2.1268\n",
      "Epoch [28/50] | D Loss: 0.2107 | G Loss: 2.7023\n",
      "Epoch [29/50] | D Loss: 0.3552 | G Loss: 2.7823\n",
      "Epoch [30/50] | D Loss: 0.3475 | G Loss: 2.9280\n",
      "Epoch [31/50] | D Loss: 0.2872 | G Loss: 2.4098\n",
      "Epoch [32/50] | D Loss: 0.3177 | G Loss: 2.4853\n",
      "Epoch [33/50] | D Loss: 0.4461 | G Loss: 1.8567\n",
      "Epoch [34/50] | D Loss: 0.4406 | G Loss: 1.9339\n",
      "Epoch [35/50] | D Loss: 0.2122 | G Loss: 2.5356\n",
      "Epoch [36/50] | D Loss: 0.4401 | G Loss: 2.3024\n",
      "Epoch [37/50] | D Loss: 0.3604 | G Loss: 1.6847\n",
      "Epoch [38/50] | D Loss: 0.4217 | G Loss: 1.5111\n",
      "Epoch [39/50] | D Loss: 0.4511 | G Loss: 1.5438\n",
      "Epoch [40/50] | D Loss: 0.2995 | G Loss: 2.3122\n",
      "Epoch [41/50] | D Loss: 0.3671 | G Loss: 1.7664\n",
      "Epoch [42/50] | D Loss: 0.3498 | G Loss: 2.2960\n",
      "Epoch [43/50] | D Loss: 0.5290 | G Loss: 1.6091\n",
      "Epoch [44/50] | D Loss: 0.2287 | G Loss: 2.1279\n",
      "Epoch [45/50] | D Loss: 0.4877 | G Loss: 1.8513\n",
      "Epoch [46/50] | D Loss: 0.3813 | G Loss: 1.7051\n",
      "Epoch [47/50] | D Loss: 0.5489 | G Loss: 1.1429\n",
      "Epoch [48/50] | D Loss: 0.4448 | G Loss: 1.4701\n",
      "Epoch [49/50] | D Loss: 0.5447 | G Loss: 1.8453\n",
      "Epoch [50/50] | D Loss: 0.3403 | G Loss: 2.0710\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "latent_dim = 100\n",
    "num_classes = 10  # For example, MNIST has 10 digit classes\n",
    "img_shape = (1, 28, 28)\n",
    "batch_size = 64\n",
    "lr = 0.0002\n",
    "\n",
    "# Initialize models\n",
    "generator = Generator(latent_dim, num_classes, img_shape).to(device)\n",
    "discriminator = Discriminator(num_classes, img_shape).to(device)\n",
    "\n",
    "# Loss function\n",
    "adversarial_loss = nn.BCELoss()\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=lr)\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr)\n",
    "\n",
    "# Load dataset (MNIST)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "dataset = torchvision.datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (imgs, labels) in enumerate(dataloader):\n",
    "        batch_size = imgs.shape[0]\n",
    "        real_imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Train Discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "        z = torch.randn(batch_size, latent_dim).to(device)\n",
    "        gen_labels = torch.randint(0, num_classes, (batch_size,)).to(device)\n",
    "        fake_imgs = generator(z, gen_labels)\n",
    "        \n",
    "        real_validity = discriminator(real_imgs, labels)\n",
    "        fake_validity = discriminator(fake_imgs.detach(), gen_labels)\n",
    "        \n",
    "        real_loss = adversarial_loss(real_validity, torch.ones_like(real_validity))\n",
    "        fake_loss = adversarial_loss(fake_validity, torch.zeros_like(fake_validity))\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Train Generator\n",
    "        optimizer_G.zero_grad()\n",
    "        fake_validity = discriminator(fake_imgs, gen_labels)\n",
    "        g_loss = adversarial_loss(fake_validity, torch.ones_like(fake_validity))\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] | D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f258a42-8536-472e-b215-faa7934ccbb5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fake_imgs\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Generate images for validation\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m9\u001b[39m])  \u001b[38;5;66;03m# Digits 0-9 for counterfactuals\u001b[39;00m\n\u001b[0;32m      9\u001b[0m generated_imgs \u001b[38;5;241m=\u001b[39m generate_counterfactuals(generator, labels, latent_dim, img_shape, device)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Plot generated images\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "def generate_counterfactuals(generator, labels, latent_dim, img_shape, device):\n",
    "    z = torch.randn(labels.shape[0], latent_dim).to(device)\n",
    "    gen_labels = labels.to(device)\n",
    "    fake_imgs = generator(z, gen_labels)\n",
    "    return fake_imgs\n",
    "\n",
    "# Generate images for validation\n",
    "labels = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])  # Digits 0-9 for counterfactuals\n",
    "generated_imgs = generate_counterfactuals(generator, labels, latent_dim, img_shape, device)\n",
    "\n",
    "# Plot generated images\n",
    "fig, axes = plt.subplots(1, 10, figsize=(10, 2))\n",
    "for i, img in enumerate(generated_imgs):\n",
    "    axes[i].imshow(img.cpu().detach().numpy().squeeze(), cmap=\"gray\")\n",
    "    axes[i].set_title(f\"Label {labels[i].item()}\")\n",
    "    axes[i].axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca3e17c-5da9-43c6-876a-54a5836b2425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Load trained classifier\n",
    "classifier = torchvision.models.resnet18(pretrained=True)  # Example classifier\n",
    "classifier.fc = nn.Linear(classifier.fc.in_features, num_classes)\n",
    "classifier.to(device)\n",
    "classifier.eval()\n",
    "\n",
    "# Validate generated images\n",
    "with torch.no_grad():\n",
    "    outputs = classifier(generated_imgs.repeat(1, 3, 1, 1))  # Convert grayscale to 3 channels\n",
    "    predictions = torch.argmax(F.softmax(outputs, dim=1), dim=1)\n",
    "\n",
    "print(\"Generated Labels:\", labels.tolist())\n",
    "print(\"Classifier Predictions:\", predictions.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d86e9a-1387-403b-8869-a0762cedfebb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
